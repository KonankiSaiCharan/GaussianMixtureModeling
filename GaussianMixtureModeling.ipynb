{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Initialization(GMMData, numOfClusters):\n",
    "    #Creating centroids with zero values i.e., initializing\n",
    "    numOfCols = (GMMData.shape)[1]\n",
    "    covariance = list()\n",
    "    centroids = np.zeros((numOfClusters, numOfCols))\n",
    "    pi = np.ones(numOfClusters)*1.0/numOfClusters\n",
    "    #From the length of the dataset selecting random centroids \n",
    "    for numOfClusters in range(numOfClusters):\n",
    "        index = rd.randint(0,len(GMMData)-1)\n",
    "        centroids[numOfClusters] = GMMData[index]\n",
    "        covariance.append(np.cov(GMMData.T))\n",
    "    return centroids, covariance, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here, the sum of all points belonging to this cluster probabilities are calculated\n",
    "def sumOfResponsibilitiesForCluster(GMMDataX, numOfClusters, centroids, covariance, pi):\n",
    "    totalProbabilityForCluster = 0.0\n",
    "    for i in range(numOfClusters):\n",
    "        totalProbabilityForCluster += pi[i]*GMM(GMMDataX, centroids[i], covariance[i])\n",
    "    return totalProbabilityForCluster\n",
    "\n",
    "#Here, we calculate the individual probabilities of each point for a different cluster\n",
    "def ExpectationStep(GMMData, numOfClusters, centroids, covariance, pi):  \n",
    "    probabilities = np.zeros((len(GMMData), numOfClusters))\n",
    "    for index in range(len(GMMData)):\n",
    "        for j in range(numOfClusters):\n",
    "            probabilities[index][j] = pi[j] * GMM(GMMData[index], centroids[j], \n",
    "                                                       covariance[j])/sumOfResponsibilitiesForCluster(GMMData[index], numOfClusters,\n",
    "                                                                                                                  centroids, covariance, pi)\n",
    "    return probabilities\n",
    "\n",
    "def l_l(GMMData, numOfClusters, centroids, covariance, pi):\n",
    "    l_l = 0.0\n",
    "    for x in range (len(GMMData)):\n",
    "        l_l += np.log(sumOfResponsibilitiesForCluster(GMMData[x], numOfClusters, centroids, covariance, pi))\n",
    "    return l_l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MaximizationStep(GMMData, numOfClusters, probabilities):\n",
    "    \n",
    "    #Getting the dimensions of the data\n",
    "    NumOfCols = GMMData.shape[1]\n",
    "    numOfRows = GMMData.shape[0]\n",
    "    \n",
    "    #Initializing the centroids to 0\n",
    "    centroids = np.zeros((numOfClusters, NumOfCols))\n",
    "    covariance = np.zeros((numOfClusters, NumOfCols, NumOfCols))\n",
    "    pi = np.zeros(numOfClusters)\n",
    "    \n",
    "    #Total probability for eacch cluster is stored in the below variable\n",
    "    TotalProbForCluster = np.zeros(numOfClusters)\n",
    "    #By using the prob. values we compute mean, variance and pi using the repective formulae\n",
    "    for j in range(numOfClusters):\n",
    "        for i in range(numOfRows):\n",
    "            TotalProbForCluster[j] += probabilities[i][j]\n",
    "            centroids[j] += (probabilities[i][j])*GMMData[i]\n",
    "        centroids[j] = centroids[j]/TotalProbForCluster[j]\n",
    "\n",
    "        for index in range(numOfRows):\n",
    "            diffDataCentroids = np.zeros((1,NumOfCols))+GMMData[index]-centroids[j]\n",
    "            covariance[j] += (probabilities[index][j]/TotalProbForCluster[j])*diffDataCentroids*diffDataCentroids.T\n",
    "        pi[j] = TotalProbForCluster[j]/numOfRows  \n",
    "        \n",
    "    return centroids, covariance, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Here , we obtain the prob. distribution function of gaussian\n",
    "def GMM(row, centroids, covariance):\n",
    "    size = len(row)\n",
    "    Normalizevalue = (2*np.pi)**size\n",
    "    Normalizevalue *= np.linalg.det(covariance)\n",
    "    Normalizevalue = 1.0/np.sqrt(Normalizevalue)\n",
    "    row_centroids = np.matrix(row-centroids)\n",
    "    x = Normalizevalue*np.exp(-0.5*row_centroids*np.linalg.inv(covariance)*row_centroids.T)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the whole EM loop where we run the E step and the M step repeatedly\n",
    "def LoopForExpMax(GMMData, numOfClusters, cutoff, Iterations):\n",
    "    #Take the initial parameters\n",
    "    IterationLimit = Iterations\n",
    "    #Initialize the initial parameters from the above functions\n",
    "    centroids, covariance, pi = Initialization(GMMData, numOfClusters)\n",
    "    #Calculate the initial log likelihood\n",
    "    InitialL_L = l_l(GMMData, numOfClusters, centroids, covariance, pi)\n",
    "    \n",
    "    #Run through the loop until you get the cutoff value\n",
    "    for i in range(IterationLimit):\n",
    "        #Get the probabilities or responsibilities from the expectation step\n",
    "        probabilities = ExpectationStep(GMMData, numOfClusters, centroids, covariance, pi)\n",
    "        #Get the updated centroids and covariances from the maximization step\n",
    "        centroids, covariance, pi = MaximizationStep(GMMData, numOfClusters, probabilities)\n",
    "        #Updated log likelihood\n",
    "        updatedL_L = l_l(GMMData, numOfClusters, centroids, covariance, pi)\n",
    "        #Compare the change with the threshold \n",
    "        if (abs(updatedL_L-InitialL_L) < cutoff):\n",
    "            break\n",
    "        InitialL_L = updatedL_L\n",
    "    counts = np.sum(probabilities,axis=0)\n",
    "    return centroids, covariance, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data2Gaussiann=np.loadtxt(\"C:\\\\Users\\\\saich\\\\Desktop\\\\UnsupervisedML\\\\2gaussian.txt\")\n",
    "Data3Gaussiann=np.loadtxt(\"C:\\\\Users\\\\saich\\\\Desktop\\\\UnsupervisedML\\\\3gaussian.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2Result = LoopForExpMax(GMMData=Data2Gaussiann,Iterations=1000,cutoff=0.1,numOfClusters=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 7.01612695,  3.98483588],\n",
       "        [ 2.99992428,  3.05143718]]), array([[[ 0.96960639,  0.49454811],\n",
       "         [ 0.49454811,  0.99866349]],\n",
       " \n",
       "        [[ 1.01987127,  0.02619693],\n",
       "         [ 0.02619693,  2.93334223]]]), array([ 3985.36809312,  2014.63190688]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data3Result = LoopForExpMax(GMMData=Data3Gaussiann,Iterations=1000,cutoff=0.1,numOfClusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 5.02000061,  7.00869488],\n",
       "        [ 7.02506026,  4.0173547 ],\n",
       "        [ 3.0569233 ,  3.09710691]]), array([[[ 0.96982309,  0.17920036],\n",
       "         [ 0.17920036,  0.96484866]],\n",
       " \n",
       "        [[ 0.98453844,  0.49873805],\n",
       "         [ 0.49873805,  0.99547484]],\n",
       " \n",
       "        [[ 1.04189909,  0.05966726],\n",
       "         [ 0.05966726,  3.48394469]]]), array([ 4924.57997498,  2979.823429  ,  2095.59659602]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Obtaining the fashion data to perform analysis\n",
    "def getFashionData(path, kind='train'):\n",
    "    import numpy as np\n",
    "    import gzip\n",
    "    import os\n",
    "    \n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    \n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fashionDataValues, fashionDataLabels = getFashionData(\"C:\\\\Users\\\\saich\\\\Desktop\\\\UnsupervisedML\\\\fashion-mnist-master\\\\data\\\\fashion\", kind='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Give the num of components and covariance matrix type\n",
    "GaussianMixtures = GaussianMixture(n_components = 10, covariance_type = 'diag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianMixture(covariance_type='diag', init_params='kmeans', max_iter=100,\n",
       "        means_init=None, n_components=10, n_init=1, precisions_init=None,\n",
       "        random_state=None, reg_covar=1e-06, tol=0.001, verbose=0,\n",
       "        verbose_interval=10, warm_start=False, weights_init=None)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit the model created\n",
    "GaussianMixtures.fit(fashionDataValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          9.71271441e+00,   4.62258760e+00,   6.99658666e-01],\n",
       "       [  0.00000000e+00,   2.83971376e-02,   2.10642059e-01, ...,\n",
       "          1.88201131e+01,   3.44755629e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00, ...,\n",
       "          8.74406780e+00,   3.96398911e+00,   2.56982145e-01],\n",
       "       ..., \n",
       "       [  0.00000000e+00,   0.00000000e+00,   1.06489196e-01, ...,\n",
       "          1.59258387e-02,   0.00000000e+00,   0.00000000e+00],\n",
       "       [  0.00000000e+00,   1.75330394e-02,   7.55262622e-02, ...,\n",
       "          3.21933475e+00,   1.20168687e+00,   8.42927819e-02],\n",
       "       [  3.26289636e-02,   1.64504358e-01,   4.47969162e-01, ...,\n",
       "          7.18788980e+00,   4.61971812e+00,   9.46920397e-01]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GaussianMixtures.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00000000e-06,   1.00000000e-06,   1.00000000e-06, ...,\n",
       "          1.18401767e+03,   5.25119008e+02,   6.20265207e+01],\n",
       "       [  1.00000000e-06,   6.28185692e-02,   4.84752272e-01, ...,\n",
       "          1.76337250e+03,   2.18078331e+02,   1.00000000e-06],\n",
       "       [  1.00000000e-06,   1.00000000e-06,   1.00000000e-06, ...,\n",
       "          1.04475034e+03,   4.22797927e+02,   1.32715629e+01],\n",
       "       ..., \n",
       "       [  1.00000000e-06,   1.00000000e-06,   1.98311655e-01, ...,\n",
       "          1.99517892e-02,   1.00000000e-06,   1.00000000e-06],\n",
       "       [  1.00000000e-06,   3.20622806e-02,   1.38884641e+00, ...,\n",
       "          3.72982300e+02,   1.00718271e+02,   1.76440570e+00],\n",
       "       [  3.48338170e-01,   2.35077504e+00,   2.08783163e+01, ...,\n",
       "          8.10277262e+02,   4.97944084e+02,   4.43943850e+01]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GaussianMixtures.covariances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03030049,  0.04636618,  0.09488338,  0.13774996,  0.21326667,\n",
       "        0.18818333,  0.1699    ,  0.07011666,  0.02471524,  0.02451809])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GaussianMixtures.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictedLabels = GaussianMixtures.predict(fashionDataValues[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GIImp = list()\n",
    "maxCluster = list()\n",
    "clusterDensity = list()\n",
    "fashionDataLabels2 = fashionDataLabels[:1000]\n",
    "for IndexOfCluster in range(10):\n",
    "        Flag = (predictedLabels == IndexOfCluster)\n",
    "        clusterDensity.append(sum(np.bincount(fashionDataLabels2[Flag])))\n",
    "        GI = 0\n",
    "        for i in range(len((np.bincount(fashionDataLabels2[Flag])))):\n",
    "            GI += (((np.bincount(fashionDataLabels2[Flag]))[i])/sum(np.bincount(fashionDataLabels2[Flag]))) ** 2\n",
    "        GIImp.append(1 - GI)\n",
    "        maximum = np.argmax(np.bincount(fashionDataLabels2[Flag]))\n",
    "        maxCluster.append(np.bincount(fashionDataLabels2[Flag]).max())\n",
    "maxCluster_sum = sum(maxCluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "purityValue = maxCluster_sum/len(fashionDataLabels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49199999999999999"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purityValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GiniUnits = 0\n",
    "sumValue = 0\n",
    "for i in range(10):\n",
    "    sumValue += GIImp[i]*clusterDensity[i]\n",
    "GiniValue = sumValue/sum(clusterDensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62810521242994832"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GiniValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hence, the purity and gini index obtained using Gaussian mixtures is 0.5 and 0.63."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
